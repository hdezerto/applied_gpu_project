{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZjGrTN6dVPJN",
        "2JcNvkJUVTTO",
        "qWrVADioVgss",
        "U7d_yqGbVpgt",
        "fQX9N4E5VzUG",
        "D_Qhx3jeV90m"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DD2360 – Acceleration of LLM.c Inference\n",
        "**Project Proposal Implementation**\n",
        "\n",
        "Group members:  \n",
        "- Diogo Paulo  \n",
        "- Hugo Dezerto  \n",
        "- Maria Carolina Sebastião  \n",
        "\n",
        "Platform: Google Colab (NVIDIA Tesla T4)  \n"
      ],
      "metadata": {
        "id": "vLkTzQqPVFqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Environment Setup\n",
        "This section sets up the Google Colab environment and verifies GPU availability.\n"
      ],
      "metadata": {
        "id": "ZjGrTN6dVPJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/DD2360_project\n",
        "%cd /content/DD2360_project\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOgkRCg7J6NG",
        "outputId": "1fe47c20-4d3d-4fc8-e219-7b649f8f569a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DD2360_project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Repository Setup (llm.c)\n",
        "Clone the llm.c repository and verify file structure.\n"
      ],
      "metadata": {
        "id": "2JcNvkJUVTTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/karpathy/llm.c\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kp-mFuXKG0C",
        "outputId": "42025704-86b6-4c95-b74a-a5d75d195454"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llm.c'...\n",
            "remote: Enumerating objects: 6149, done.\u001b[K\n",
            "remote: Total 6149 (delta 0), reused 0 (delta 0), pack-reused 6149 (from 1)\u001b[K\n",
            "Receiving objects: 100% (6149/6149), 2.25 MiB | 5.44 MiB/s, done.\n",
            "Resolving deltas: 100% (3963/3963), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd llm.c\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xayGN0fTKMel",
        "outputId": "bedeaa2c-822c-4239-db7b-1df65d971261"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DD2360_project/llm.c\n",
            "dev\t  profile_gpt2.cu    test_gpt2.c\ttrain_gpt2_fp32.cu\n",
            "doc\t  profile_gpt2cu.py  test_gpt2.cu\ttrain_gpt2.py\n",
            "LICENSE   README.md\t     test_gpt2_fp32.cu\ttrain_llama3.py\n",
            "llmc\t  requirements.txt   train_gpt2.c\n",
            "Makefile  scripts\t     train_gpt2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Building the CUDA FP32 Baseline\n",
        "Compile the FP32 CUDA inference binary using the provided Makefile.\n"
      ],
      "metadata": {
        "id": "qWrVADioVgss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!make test_gpt2fp32cu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfmlwYnKKWA_",
        "outputId": "62d6681b-ccbd-4178-fef9-4768ca09a454"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------\n",
            "→ cuDNN is manually disabled by default, run make with `USE_CUDNN=1` to try to enable\n",
            "✓ OpenMP found\n",
            "✓ NCCL found, OK to train with multiple GPUs\n",
            "✓ MPI enabled\n",
            "✓ nvcc found, including GPU/CUDA support\n",
            "---------------------------------------------\n",
            "/usr/local/cuda/bin/nvcc --threads=0 -t=0 --use_fast_math -std=c++17 -O3 -DMULTI_GPU -DUSE_MPI test_gpt2_fp32.cu -lcublas -lcublasLt -lnvidia-ml -L/usr/lib/x86_64-linux-gnu/openmpi/lib/ -I/usr/lib/x86_64-linux-gnu/openmpi/include/ -lnccl -lmpi -o test_gpt2fp32cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./test_gpt2fp32cu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcTeUWRRcugL",
        "outputId": "35a77bcd-c863-4813-d48d-bd35d00d75c0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[System]\n",
            "Device 0: Tesla T4\n",
            "enable_tf32: 0\n",
            "Error: Failed to open file 'gpt2_124M.bin' at train_gpt2_fp32.cu:1119\n",
            "Error details:\n",
            "  File: train_gpt2_fp32.cu\n",
            "  Line: 1119\n",
            "  Path: gpt2_124M.bin\n",
            "  Mode: rb\n",
            "---> HINT 1: dataset files/code have moved to dev/data recently (May 20, 2024). You may have to mv them from the legacy data/ dir to dev/data/(dataset), or re-run the data preprocessing script. Refer back to the main README\n",
            "---> HINT 2: possibly try to re-run `python train_gpt2.py`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model and Data Preparation\n",
        "Generate the required dataset and convert pretrained GPT-2 weights. Just doing this because it was missing this file \"gpt2_124M.bin\""
      ],
      "metadata": {
        "id": "U7d_yqGbVpgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python dev/data/tinyshakespeare.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iNv52wpPhLs",
        "outputId": "ea11ca18-440b-4234-b847-7f1954ab34f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt to /content/DD2360_project/llm.c/dev/data/tinyshakespeare/tiny_shakespeare.txt...\n",
            "\r/content/DD2360_project/llm.c/dev/data/tinyshakespeare/tiny_shakespeare.txt:   0% 0.00/425k [00:00<?, ?iB/s]\r/content/DD2360_project/llm.c/dev/data/tinyshakespeare/tiny_shakespeare.txt: 1.06MiB [00:00, 44.0MiB/s]     \n",
            "writing 32,768 tokens to /content/DD2360_project/llm.c/dev/data/tinyshakespeare/tiny_shakespeare_val.bin (66,560 bytes) in the gpt-2 format\n",
            "writing 305,260 tokens to /content/DD2360_project/llm.c/dev/data/tinyshakespeare/tiny_shakespeare_train.bin (611,544 bytes) in the gpt-2 format\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_gpt2.py --write_tensors 1 --model gpt2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeyrwWLIPoLl",
        "outputId": "905060dc-cbb5-46ca-c13b-6a10c5279e1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running pytorch 2.9.0+cu126\n",
            "using device: cuda\n",
            "total desired batch size: 256\n",
            "=> calculated gradient accumulation steps: 1\n",
            "wrote gpt2_tokenizer.bin\n",
            "2025-12-30 17:11:53.378779: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1767114713.399220    8083 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1767114713.405355    8083 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1767114713.423678    8083 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1767114713.423702    8083 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1767114713.423706    8083 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1767114713.423710    8083 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-30 17:11:53.428305: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "loading weights from pretrained gpt: gpt2\n",
            "DataLoader: total number of tokens: 32,768 across 1 files\n",
            "padded vocab size from 50257 to 50304\n",
            "wrote gpt2_124M.bin\n",
            "padded vocab size from 50257 to 50304\n",
            "wrote gpt2_124M_bf16.bin\n",
            "padded vocab size in reference grads from 50257 to 50304\n",
            "wrote gpt2_124M_debug_state.bin\n",
            "num decayed parameter tensors: 50, with 124,318,464 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n",
            "using regular AdamW\n",
            "step    1/10 | train loss 5.270008 | norm 30.5001 | lr 1.00e-04 | (319.05 ms | 802 tok/s)\n",
            "step    2/10 | train loss 4.060704 | norm 17.0766 | lr 1.00e-04 | (137.77 ms | 1858 tok/s)\n",
            "step    3/10 | train loss 3.320132 | norm 14.7814 | lr 1.00e-04 | (98.03 ms | 2612 tok/s)\n",
            "step    4/10 | train loss 2.717582 | norm 13.1940 | lr 1.00e-04 | (97.23 ms | 2633 tok/s)\n",
            "step    5/10 | train loss 2.181075 | norm 12.3861 | lr 1.00e-04 | (105.16 ms | 2434 tok/s)\n",
            "step    6/10 | train loss 1.653916 | norm 10.6285 | lr 1.00e-04 | (92.22 ms | 2776 tok/s)\n",
            "step    7/10 | train loss 1.168059 | norm 9.7809 | lr 1.00e-04 | (94.85 ms | 2699 tok/s)\n",
            "step    8/10 | train loss 0.736802 | norm 8.1160 | lr 1.00e-04 | (94.57 ms | 2707 tok/s)\n",
            "step    9/10 | train loss 0.400922 | norm 6.2661 | lr 1.00e-04 | (94.95 ms | 2696 tok/s)\n",
            "step   10/10 | train loss 0.187426 | norm 3.6634 | lr 1.00e-04 | (94.77 ms | 2701 tok/s)\n",
            "final 9 iters avg: 101.062ms\n",
            "peak memory consumption: 2321 MiB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. CUDA Architecture Compatibility Fix\n",
        "Resolve PTX toolchain mismatch by compiling specifically for Tesla T4 (compute capability 7.5).\n"
      ],
      "metadata": {
        "id": "fQX9N4E5VzUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!make clean\n",
        "!make test_gpt2fp32cu GPU_COMPUTE_CAPABILITY=75"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SjWqc9PQ5vs",
        "outputId": "884ca562-d4fe-4a34-f89e-c285d77dee43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------\n",
            "→ cuDNN is manually disabled by default, run make with `USE_CUDNN=1` to try to enable\n",
            "✓ OpenMP found\n",
            "✓ NCCL found, OK to train with multiple GPUs\n",
            "✓ MPI enabled\n",
            "✓ nvcc found, including GPU/CUDA support\n",
            "---------------------------------------------\n",
            "rm -f train_gpt2 test_gpt2 train_gpt2cu test_gpt2cu train_gpt2fp32cu test_gpt2fp32cu \n",
            "rm -f build/*.o\n",
            "---------------------------------------------\n",
            "→ cuDNN is manually disabled by default, run make with `USE_CUDNN=1` to try to enable\n",
            "✓ OpenMP found\n",
            "✓ NCCL found, OK to train with multiple GPUs\n",
            "✓ MPI enabled\n",
            "✓ nvcc found, including GPU/CUDA support\n",
            "---------------------------------------------\n",
            "/usr/local/cuda/bin/nvcc --threads=0 -t=0 --use_fast_math -std=c++17 -O3 --generate-code arch=compute_75,code=[compute_75,sm_75] -DMULTI_GPU -DUSE_MPI test_gpt2_fp32.cu -lcublas -lcublasLt -lnvidia-ml -L/usr/lib/x86_64-linux-gnu/openmpi/lib/ -I/usr/lib/x86_64-linux-gnu/openmpi/include/ -lnccl -lmpi -o test_gpt2fp32cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Baseline Inference Validation (FP32)\n",
        "Run the CUDA FP32 inference and verify numerical correctness.\n"
      ],
      "metadata": {
        "id": "D_Qhx3jeV90m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./test_gpt2fp32cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGZkPK2gLZMs",
        "outputId": "2e3d6f9c-d9a5-4a88-c47e-5b77665e3c1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[System]\n",
            "Device 0: Tesla T4\n",
            "enable_tf32: 0\n",
            "[State]\n",
            "batch_size: 4\n",
            "seq_len: 64\n",
            "allocated 221 MiB for activations\n",
            "-43.431618, -43.431633\n",
            "-39.836346, -39.836357\n",
            "-43.065903, -43.065926\n",
            "-42.828041, -42.828056\n",
            "-43.529537, -43.529564\n",
            "-44.318390, -44.318409\n",
            "-41.227409, -41.227428\n",
            "-41.270756, -41.270779\n",
            "-42.541397, -42.541420\n",
            "-42.394989, -42.395012\n",
            "OK (LOGITS)\n",
            "allocated 474 MiB for parameter gradients\n",
            "allocated 4 MiB for activation gradients\n",
            "LOSS OK: 5.270009 5.270008\n",
            "grads\n",
            "OK -0.002320 -0.002320\n",
            "OK 0.002072 0.002072\n",
            "OK 0.003717 0.003717\n",
            "OK 0.001307 0.001307\n",
            "OK 0.000632 0.000632\n",
            "TENSOR OK\n",
            "allocated 474 MiB for AdamW optimizer state m\n",
            "allocated 474 MiB for AdamW optimizer state v\n",
            "step 0: loss 5.270009 (took 67.082373 ms)\n",
            "step 1: loss 4.059695 (took 55.237933 ms)\n",
            "step 2: loss 3.375108 (took 86.675907 ms)\n",
            "step 3: loss 2.800755 (took 76.870364 ms)\n",
            "step 4: loss 2.315362 (took 74.005729 ms)\n",
            "step 5: loss 1.849014 (took 72.937497 ms)\n",
            "step 6: loss 1.394637 (took 74.148315 ms)\n",
            "step 7: loss 0.999126 (took 74.386308 ms)\n",
            "step 8: loss 0.624076 (took 72.562472 ms)\n",
            "step 9: loss 0.376510 (took 73.667893 ms)\n",
            "loss ok at step 0: 5.270009 5.270007\n",
            "loss ok at step 1: 4.059695 4.059707\n",
            "loss ok at step 2: 3.375108 3.375123\n",
            "loss ok at step 3: 2.800755 2.800783\n",
            "loss ok at step 4: 2.315362 2.315382\n",
            "loss ok at step 5: 1.849014 1.849029\n",
            "loss ok at step 6: 1.394637 1.394656\n",
            "loss ok at step 7: 0.999126 0.999147\n",
            "loss ok at step 8: 0.624076 0.624080\n",
            "loss ok at step 9: 0.376510 0.376511\n",
            "overall okay: 1\n"
          ]
        }
      ]
    }
  ]
}